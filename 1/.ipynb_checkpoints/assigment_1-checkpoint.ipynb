{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8636ee53",
   "metadata": {},
   "source": [
    "# Assignment 1:\n",
    "The assignment consists in the development, in NLTK, OpenNLP, SketchEngine or GATE/Annie a Naïve Bayes Classifier able to detect a single class in one of the corpora available as attachments to the chosen package, by distinguishing ENGLISH against NON-ENGLISH. In particular the classifier has to be:\n",
    "\n",
    "- Trained on a split subset of the chosen corpus, by either using an existing partition between sample documents for training and for test or by using a random splitter among the available ones;\n",
    "\n",
    "- Devised as a pipeline of any chosen format, including the simplest version based on word2vec on a list of words obtained by one of the available lexical resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9762502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************ IMPORTS ************************#\n",
    "import nltk\n",
    "import asyncio\n",
    "import random\n",
    "import math \n",
    "import collections\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0bc21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "# CORPUS DATA \n",
    "en_ids = [fileid for fileid in europarl_raw.english.fileids()]\n",
    "dutch_ids = [fileid for fileid in europarl_raw.dutch.fileids()]\n",
    "fr_ids = [fileid for fileid in europarl_raw.french.fileids()]\n",
    "\n",
    "# Loading ENGLISH corpora and respective label \n",
    "documents= [(europarl_raw.english.raw(fileid), \"English\") \n",
    "            for fileid in en_ids]\n",
    "# Loading FRENCH corpora and respective label \n",
    "for fileid in fr_ids:\n",
    "    documents.append((europarl_raw.french.raw(fileid) , \"NonEnglish\"))\n",
    "# Loading DUTCH corpora and respective label \n",
    "for fileid in dutch_ids:\n",
    "    documents.append((europarl_raw.dutch.raw(fileid) , \"NonEnglish\"))\n",
    "    \n",
    "random.shuffle(documents)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8efeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOPWORDS \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))  \n",
    "stop_words.add(word for word in stopwords.words(\"french\"))\n",
    "stop_words.add(word for word in stopwords.words(\"dutch\"))\n",
    "\n",
    "# STEMMER \n",
    "from nltk.stem import PorterStemmer \n",
    "stemmer = PorterStemmer()  \n",
    "\n",
    "# LEMMATIZER\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c93b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:47<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZATION, LEMMATIZING, STEMMMING AND STOP WORDS REMOVAL \n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "fdist = FreqDist() # freqdist to keep counting w instances for creating BOW \n",
    "data = [0 for _ in range(len(documents))]\n",
    "\n",
    "for i,(text,label) in enumerate(tqdm(documents)):\n",
    "    appo = ([],label)\n",
    "\n",
    "    sents = sent_tokenize(text)\n",
    "    for sent in sents:\n",
    "        words = word_tokenize(sent) \n",
    "        for word in words:\n",
    "            if word.casefold() not in stop_words:\n",
    "                stemmed = stemmer.stem(word.lower()) # Stemming \n",
    "                lemmatized = lemmatizer.lemmatize(stemmed) # Lemmatization\n",
    "                fdist[lemmatized] += 1 # Increases Word Counter inside the Bag of Words\n",
    "                appo[0].append(lemmatized) # Saves the Result\n",
    "\n",
    "    data[i] = appo \n",
    "\n",
    "top_words = list(fdist)[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b71454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:00<00:00, 218.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction: \n",
    "def feature_estractor(document,top_words):\n",
    "    document_set = set(document)\n",
    "    features = {}\n",
    "    for word in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_set)\n",
    "    return features\n",
    "\n",
    "featuresets = [(feature_estractor(d,top_words), c) for (d,c) in tqdm(data)]\n",
    "train_test_split = math.floor(len(featuresets) * 0.7 )\n",
    "train_set, test_set = featuresets[:train_test_split], featuresets[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6cdc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec162839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing and Metrics: \n",
      "True value: NonEnglish Our value: NonEnglish\n",
      "True value: NonEnglish Our value: NonEnglish\n",
      "True value: NonEnglish Our value: NonEnglish\n",
      "True value: NonEnglish Our value: NonEnglish\n",
      "True value: NonEnglish Our value: NonEnglish\n",
      "True value: NonEnglish Our value: NonEnglish\n",
      "True value: English Our value: NonEnglish\n",
      "True value: NonEnglish Our value: NonEnglish\n",
      "True value: English Our value: English\n",
      "Accuracy: 0.8888888888888888\n",
      "Precision: 1.0\n",
      "Recall: 0.5\n",
      "           |      N        |\n",
      "           |      o        |\n",
      "           |      n        |\n",
      "           |      E      E |\n",
      "           |      n      n |\n",
      "           |      g      g |\n",
      "           |      l      l |\n",
      "           |      i      i |\n",
      "           |      s      s |\n",
      "           |      h      h |\n",
      "-----------+---------------+\n",
      "NonEnglish | <77.8%>     . |\n",
      "   English |  11.1% <11.1%>|\n",
      "-----------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Most Informative Features\n",
      "         contains(alway) = True           Englis : NonEng =      8.8 : 1.0\n",
      "        contains(author) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(back) = True           Englis : NonEng =      8.8 : 1.0\n",
      "        contains(common) = True           Englis : NonEng =      8.8 : 1.0\n",
      "        contains(compet) = True           Englis : NonEng =      8.8 : 1.0\n",
      "        contains(confer) = True           Englis : NonEng =      8.8 : 1.0\n",
      "       contains(connect) = True           Englis : NonEng =      8.8 : 1.0\n",
      "       contains(constat) = False          Englis : NonEng =      8.8 : 1.0\n",
      "       contains(council) = True           Englis : NonEng =      8.8 : 1.0\n",
      "         contains(cours) = True           Englis : NonEng =      8.8 : 1.0\n",
      "      contains(culturel) = False          Englis : NonEng =      8.8 : 1.0\n",
      "          contains(deal) = True           Englis : NonEng =      8.8 : 1.0\n",
      "        contains(effici) = True           Englis : NonEng =      8.8 : 1.0\n",
      "           contains(end) = True           Englis : NonEng =      8.8 : 1.0\n",
      "       contains(essenti) = True           Englis : NonEng =      8.8 : 1.0\n",
      "      contains(european) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(fact) = True           Englis : NonEng =      8.8 : 1.0\n",
      "           contains(far) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(find) = True           Englis : NonEng =      8.8 : 1.0\n",
      "         contains(first) = True           Englis : NonEng =      8.8 : 1.0\n",
      "         contains(great) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(help) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(high) = True           Englis : NonEng =      8.8 : 1.0\n",
      "        contains(invest) = True           Englis : NonEng =      8.8 : 1.0\n",
      "         contains(least) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(left) = True           Englis : NonEng =      8.8 : 1.0\n",
      "         contains(legal) = True           Englis : NonEng =      8.8 : 1.0\n",
      "         contains(level) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(mani) = True           Englis : NonEng =      8.8 : 1.0\n",
      "            contains(me) = False          Englis : NonEng =      8.8 : 1.0\n",
      "          contains(meet) = True           Englis : NonEng =      8.8 : 1.0\n",
      "       contains(mondial) = False          Englis : NonEng =      8.8 : 1.0\n",
      "     contains(particuli) = False          Englis : NonEng =      8.8 : 1.0\n",
      "         contains(peopl) = True           Englis : NonEng =      8.8 : 1.0\n",
      "          contains(play) = True           Englis : NonEng =      8.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.scores import (precision, recall)\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "print(\"Testing and Metrics: \")\n",
    "refsets =  collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "labels = []\n",
    "tests = []\n",
    "for i,(feats,label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    result = classifier.classify(feats)\n",
    "    testsets[result].add(i)\n",
    "    labels.append(label)\n",
    "    tests.append(result)\n",
    "    print(\"True value: \"+label+\" Our value: \"+result)\n",
    "    \n",
    "cm = ConfusionMatrix(labels, tests)\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n",
    "print( 'Precision:', precision(refsets['English'], testsets['English']) )\n",
    "print( 'Recall:', recall(refsets['English'], testsets['English']) )\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
    "classifier.show_most_informative_features(35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37917340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NonEnglish NonEnglish\n",
      "NonEnglish NonEnglish\n",
      "NonEnglish NonEnglish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = test_set[1]\n",
    "print(classifier.classify(test[0]),test[1])\n",
    "test = test_set[3]\n",
    "print(classifier.classify(test[0]),test[1])\n",
    "test = test_set[0]\n",
    "print(classifier.classify(test[0]),test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40aad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
