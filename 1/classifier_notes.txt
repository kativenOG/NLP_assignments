\section{FIRST ASSIGNMENT:}
- Tokenisation 
- Chunking 
- Stop words elimination (im not really sure about this) it flattens the curve on n and 
- Stemming !!! don't do it on less than 4 characters, it's a suicide otherwhise and ruins the corpus 
- Lemmatization 
- Creating the Bows
BOW = tf/cf ( text frequency and corpus frequency ) => hte higher it is, the more it's related to the specific text.
- Stats

\footnote{N.B.: A word that appears erlier in a tetxt / paragraph, it's probably more important.}

\subsubsection{Logistical Regression:}
#Licker scale: 
a monodimensional statistical scale used to rappresent features. 
For each feature (word) has a weight (importance based on #appearence) and bias.
To make the regressor we sum up all the weighted features amd the bias.
We use logistical regression because the outcome is a probability ([0,1]) and not a Real number:
Y = 1/ (1 + e^-z) => \sigma(w*x +b)

\subsubsection{NB:}
- Logistical regression is going to work in a lot of cases. 
- Rappresents Bayes theorem because it's it's complement !! 
- 0.5 is called the decision boundary, we can approach it in different ways, some simpler, some more complex. It's a fuzzy value.
- It's a lot easier because we go directly to the result, sentiment analisys it's a lot simpler  

