{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8636ee53",
   "metadata": {},
   "source": [
    "# Assignment 1: Classifing Documents\n",
    "The assignment consists in the development, in NLTK, OpenNLP, SketchEngine or GATE/Annie a Naïve Bayes Classifier able to detect a single class in one of the corpora available as attachments to the chosen package, by distinguishing ENGLISH against NON-ENGLISH. In particular the classifier has to be:\n",
    "\n",
    "- Trained on a split subset of the chosen corpus, by either using an existing partition between sample documents for training and for test or by using a random splitter among the available ones;\n",
    "\n",
    "- Devised as a pipeline of any chosen format, including the simplest version based on word2vec on a list of words obtained by one of the available lexical resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b38c8",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "I decided to use only NLTK for defining the pipeline, fetching the data and analize the results. I decided to train the classifier on full documents, so the results on tests sets are flawless (if not perfect) in most of the trainings. </br>\n",
    "The catch is that this classifier should be used only with documents of the same size or bigger, smaller documents (or even only paragraphs) are unreliably detected by the classifier, and therefore should be avoided. </br>\n",
    "The script can be divided in five distinct parts:\n",
    "- ***Data Fetching***\n",
    "- ***Pipeline***\n",
    "- ***Feature Extraction***\n",
    "- ***Traning the Model***\n",
    "- ***Analyzing the Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9762502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************ IMPORTS ************************#\n",
    "import nltk\n",
    "import asyncio\n",
    "import random\n",
    "import math \n",
    "import collections\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eae693",
   "metadata": {},
   "source": [
    "## Data Fetching:\n",
    "In this step documents are fetched, associated with the proper label, depending on the class, and shuffled.\n",
    "The script takes in consideration 3 languages: ***English***, ***French*** and ***Dutch***.</br>\n",
    "In this dataset consist in a total of 46 documents, of which 26 documents are in English and the other 20 in NonEnglish (i.e. Dutch and French); English documents not from the exact same background, 10 of them are from **europarl_raw** (a dataset that contains speeches of euro-parlamentars) and the other 16 are from the **state_union** dataset (American Presidents speeches). </br>\n",
    "In my opinion, even if American and British English are different, in a formal context (e.g. a politician speech) they tend to be more similar than if compared in a casual setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0bc21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Documents Count: 26\n",
      "NonEnglish Documents Count: 20\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw # Euro Parlamentars speeches \n",
    "from nltk.corpus import state_union as union # America's presidents Union Day speeches \n",
    "# CORPUS DATA \n",
    "\n",
    "# Creating iterators containing all the needed file ids\n",
    "en_ids = [fileid for fileid in europarl_raw.english.fileids()]\n",
    "dutch_ids = [fileid for fileid in europarl_raw.dutch.fileids()]\n",
    "fr_ids = [fileid for fileid in europarl_raw.french.fileids()]\n",
    "union_ids = [fileid for fileid in union.fileids()]\n",
    "union_ids = union_ids[:math.floor(len(union_ids)/4)]\n",
    "print(\"English Documents Count:\", len(en_ids)+len(union_ids))\n",
    "print(\"NonEnglish Documents Count:\", len(dutch_ids)+len(fr_ids))\n",
    "\n",
    "# Loading ENGLISH euro_parlcorpora and adding the English label \n",
    "documents= [(europarl_raw.english.raw(fileid), \"English\") for fileid in en_ids]\n",
    "\n",
    "# Loading America's union speechs corpora and adding the English label \n",
    "for fileid in union_ids:\n",
    "    documents.append((union.raw(fileid) , \"English\"))\n",
    "\n",
    "# Loading FRENCH corpora and  label \n",
    "for fileid in fr_ids:\n",
    "    documents.append((europarl_raw.french.raw(fileid) , \"NonEnglish\"))\n",
    "    \n",
    "# Loading DUTCH corpora and respective label \n",
    "for fileid in dutch_ids:\n",
    "    documents.append((europarl_raw.dutch.raw(fileid) , \"NonEnglish\"))\n",
    "    \n",
    "random.shuffle(documents)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4bde7",
   "metadata": {},
   "source": [
    "## Pipeline:\n",
    "In this step the script modifies the data through a data pipeline, with the aim of reducing the number of words in our vocabulary *V* and deleting informationless data. In this step the script also calculates the frequency of each word in the whole corpora, this is essential because in the next step this information is used to decide the features (i.e. words) of our model.\n",
    "The pipeline consist in:\n",
    "- **Tokenization**: converting text is list of sentences or and words;\n",
    "- **Stop Words removal**: removing words that dont add meaning to the text. The script uses nltk sets of stopwords for each language instead of removing the *N* most common words;\n",
    "- **Stemming**: reduce words to their root. This is done using the Porter stemmer, a really old stemmer (1979) that is also still a viable option;\n",
    "- **Lemmatizing**: reduces the words to their core meaning, but it does so by replacing the word with a word with the same meaning, instead of taking only the root like in Stemming;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8efeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))  \n",
    "stop_words.add(word for word in stopwords.words(\"french\"))\n",
    "stop_words.add(word for word in stopwords.words(\"dutch\"))\n",
    "\n",
    "# STEMMER \n",
    "from nltk.stem import PorterStemmer \n",
    "stemmer = PorterStemmer()  \n",
    "\n",
    "# LEMMATIZER\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c93b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:58<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZATION, LEMMATIZING, STEMMMING AND STOP WORDS REMOVAL \n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sentà-_tokenize, word_tokenize\n",
    "fdist = FreqDist() # freqdist to keep counting w instances for creating BOW \n",
    "data = [0 for _ in range(len(documents))]\n",
    "\n",
    "for i,(text,label) in enumerate(tqdm(documents)):\n",
    "    appo = ([],label)\n",
    "\n",
    "    sents = sent_tokenize(text)\n",
    "    for sent in sents:\n",
    "        words = word_tokenize(sent) \n",
    "        for word in words:\n",
    "            if word.casefold() not in stop_words:\n",
    "                stemmed = stemmer.stem(word.lower()) # Stemming \n",
    "                lemmatized = lemmatizer.lemmatize(stemmed) # Lemmatization\n",
    "                fdist[lemmatized] += 1 # Increases Word Counter inside the Bag of Words\n",
    "                appo[0].append(lemmatized) # Saves the Result\n",
    "\n",
    "    data[i] = appo \n",
    "top_words = list(fdist)[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e2985",
   "metadata": {},
   "source": [
    "## Feature Extraction:\n",
    "After the pipeline we transform all documents data in a readable form for the classifier; this is done by creating a dictionary (one for every document) that for each of the 2000 more common words (the most common in all the data) associates a *True* if present in the text, otherwhise a *False*. After the program has created a list with all feature set, one for each document, it just splits it two, giving 70% of the data for training, and the remaining 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b71454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 272.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction: \n",
    "def feature_estractor(document,top_words):\n",
    "    document_set = set(document)\n",
    "    features = {}\n",
    "    for word in top_words:\n",
    "        features['contains({})'.format(word)] = (word in document_set)\n",
    "    return features\n",
    "\n",
    "featuresets = [(feature_estractor(d,top_words), c) for (d,c) in tqdm(data)]\n",
    "train_test_split = math.floor(len(featuresets) * 0.7 )\n",
    "train_set, test_set = featuresets[:train_test_split], featuresets[train_test_split:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5275766c",
   "metadata": {},
   "source": [
    "## Training the Model: \n",
    "\n",
    "By using the Naive Bayes classifier every feature gets a a say in determing wich label should be assign to a given input value (in this case documents). </br>\n",
    "The Naive bayes classifier is trained on 70% of the feature sets. Thus, each feature set has to be labelled with the correct class (English or NonEnglish) by creating a tuple *(FeatureSet,Label)*. The reason for this drastic train-test split is the low ammount of documents in the corpora (only 46): during testing classifiers trained with less documents gave subpar results, especially in the most significant words department that will be  shown in the next section.\n",
    "\n",
    "\n",
    "Example of the decision process during classification of a Naive Bayes classifier:\n",
    "<img src=\"./images/naive-bayes-triangle.png\" width=\"400\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e6cdc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training Documents: 32 --- Number of testing Documents: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Number of training Documents: {len(train_set)} --- Number of testing Documents: {len(test_set)}\")\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2b77bde",
   "metadata": {},
   "source": [
    "## Analyzing the results:\n",
    "Lastly, the script computes an analysis of the results the classifier returns on testing data. The script calculates *Accuracy*, *Precision*, *Recall*, *F1 score* and creates a *Confusion Matrix*. In this **Binary classification problem** for computing the confusion matrix we need to choose one of the two labels, then extract the number of documents present in the test set for that labels  and the number of documents that are correctly classified for the same, previosly choosen label (as rappresented in the image below).</br>\n",
    "All the metrics are pretty high, this is probably related to the small size of the Corpora (the dataset has only 46 documents) and the lenght of the documents: the more a document is long, the more significant features for a language are more likely to appear. The latter is especially true in a field like politics in where much of the terminology is language dependent, unlike, for example, Computer Science corpora.\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/precision-recall.png\" width=\"700\" height=\"500\" />\n",
    "\n",
    "In the image above Relevant and Retrieved are: \n",
    "- **Relevant documents** are all the documents that are part of the positive class (English) \n",
    "- **Retrieved documents** are all  the documents that are being identified by the classifier as part of the positive class (English).\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec162839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing and Metrics: \n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 score: 1.0\n",
      "           |             N |\n",
      "           |             o |\n",
      "           |             n |\n",
      "           |      E      E |\n",
      "           |      n      n |\n",
      "           |      g      g |\n",
      "           |      l      l |\n",
      "           |      i      i |\n",
      "           |      s      s |\n",
      "           |      h      h |\n",
      "-----------+---------------+\n",
      "   English | <57.1%>     . |\n",
      "NonEnglish |      . <42.9%>|\n",
      "-----------+---------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "Most Informative Features\n",
      "        contains(achiev) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(also) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(believ) = False          NonEng : Englis =     12.2 : 1.0\n",
      "         contains(bring) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(come) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(cooper) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(cost) = False          NonEng : Englis =     12.2 : 1.0\n",
      "       contains(countri) = False          NonEng : Englis =     12.2 : 1.0\n",
      "         contains(creat) = False          NonEng : Englis =     12.2 : 1.0\n",
      "           contains(dan) = True           NonEng : Englis =     12.2 : 1.0\n",
      "            contains(du) = True           NonEng : Englis =     12.2 : 1.0\n",
      "        contains(econom) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(ever) = False          NonEng : Englis =     12.2 : 1.0\n",
      "         contains(everi) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(full) = False          NonEng : Englis =     12.2 : 1.0\n",
      "         contains(gener) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(give) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(hope) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(improv) = False          NonEng : Englis =     12.2 : 1.0\n",
      "       contains(increas) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(live) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(made) = False          NonEng : Englis =     12.2 : 1.0\n",
      "      contains(maintain) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(make) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(measur) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(member) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(much) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(must) = False          NonEng : Englis =     12.2 : 1.0\n",
      "     contains(necessari) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(need) = False          NonEng : Englis =     12.2 : 1.0\n",
      "          contains(peac) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(person) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(remain) = False          NonEng : Englis =     12.2 : 1.0\n",
      "        contains(requir) = False          NonEng : Englis =     12.2 : 1.0\n",
      "      contains(standard) = False          NonEng : Englis =     12.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.scores import (precision, recall)\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "print(\"Testing and Metrics: \")\n",
    "refsets =  collections.defaultdict(set)\n",
    "testsets = collections.defaultdict(set)\n",
    "labels = []\n",
    "tests = []\n",
    "for i,(feats,label) in enumerate(test_set):\n",
    "    refsets[label].add(i)\n",
    "    result = classifier.classify(feats)\n",
    "    testsets[result].add(i)\n",
    "    labels.append(label)\n",
    "    tests.append(result)\n",
    "    \n",
    "cm = ConfusionMatrix(labels, tests)\n",
    "print(\"Accuracy:\",nltk.classify.accuracy(classifier, test_set))\n",
    "prec = precision(refsets['English'], testsets['English'])\n",
    "print( 'Precision:', prec )\n",
    "rec = recall(refsets['English'], testsets['English'])\n",
    "print( 'Recall:', rec )\n",
    "f1 = 2 *(prec*rec)/(prec+rec)\n",
    "print(\"F1 score:\", f1)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
    "classifier.show_most_informative_features(35)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
