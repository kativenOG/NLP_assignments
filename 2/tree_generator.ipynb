{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignement 2\n",
    "\n",
    "The assignment consists in the development, in NLTK, OpenNLP, SketchEngine or GATE/Annie a pipeline that, starting from a text in input, in a given language (English, French, German and Italian are admissible) outputs the syntactic tree of the sentence itself, intended as a tree with root in S for sentence, and leaves on the tokens labelled with a single Part-of-speech. The generation of the tree can pass through one of the following models:\n",
    "\n",
    "1) PURE SYMBOLIC. The tree is generated by a LR analysis with CF LL2 grammar as a base. Candidates can assume the following:\n",
    "\n",
    "   a) Adjectives in English and German shall be only prefixed to nouns, whilst in French and Italian are only suffixed;\n",
    "\n",
    "    b) Verbs are all at present tense;\n",
    "\n",
    "    c) No pronouns are admitted;\n",
    "\n",
    "    d) Only one adverb is admitted, always post-poned with respect to the verb (independently of the language, and the type of adverb);\n",
    "\n",
    "    Overall the point above map a system that could be devised in regular expressions, but a Context-free grammar would be simpler to     \n",
    "    define. Candidate can either define a system by themselves or use a syntactic tree generation system that can be found on GitHub. \n",
    "    Same happens for POS-tagging, where some of the above mentioned systems can be customized by existing techniques that are available\n",
    "    in several fashions (including a pre-defined NLTK and OpenNLP libraries for POS-tagging and a module in GATE for the same purpose. Ambiguity \n",
    "    should be blocked onto first admissible tree.\n",
    "\n",
    "2) PURE ML. Candidates can develop a PLM with one-step Markov chains to forecast the following token, and used to generate the forecast of the\n",
    "     POS tags to be attributed. In this case the PLM can be generated starting with a Corpus, that could be obtained online, for instance by \n",
    "     using the Wikipedia access API, or other available free repos (including those available with SketchEngine. In this approach, candidates should\n",
    "     never use the forecasting to approach the determination of outcomes (for this would be identical purpose of distinguishing EN/non ENG (and\n",
    "     then IT/non IT, FR/not FR or DE/not DE) but only to identify the POS model in a sequence. In this case, the candidate should output the most\n",
    "     likely POS tagging, without associating the sequence to a tree in a direct fashion.\n",
    "\n",
    "Candidates are free to employ PURE ML approach to simplify, or pre-process the text in order to improve the performance of a PURE SYMBOLIC approach while generating a mixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /home/kativen/nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n",
      "[nltk_data] Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]     /home/kativen/nltk_data...\n",
      "[nltk_data]   Package universal_treebanks_v20 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#************************ GENERAL IMPORTS ************************#\n",
    "import spacy\n",
    "import nltk; nltk.download(\"europarl_raw\"); \n",
    "from tqdm import tqdm \n",
    "import math,random,pathlib,collections\n",
    "from nltk.corpus import europarl_raw \n",
    "spacy_to_nltk_gram = \"\"\"\n",
    "PUNCT -> \"PUNCT\"\n",
    "SCONJ -> \"SCONJ\"\n",
    "PRON -> \"PRON\"\n",
    "SYM -> \"SYM\"\n",
    "NUM -> \"NUM\"\n",
    "N -> \"NOUN\"\n",
    "V -> \"VERB\"\n",
    "AUX -> \"AUX\"\n",
    "P -> \"ADP\"\n",
    "ADJ -> \"ADJ\"\n",
    "ADV -> \"ADV\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.english.raw(europarl_raw.english.fileids()[0]))\n",
    "file = [\n",
    "    \"The fat cat is jumping.\",\n",
    "    \"The red cat is blue.\",\n",
    "    \"The cat is running away.\",\n",
    "    \"I love cats.\",\n",
    "    \"Small cats are awesome.\",\n",
    "    \"Fat cats are awesome.\"\n",
    "]\n",
    "\n",
    "# LOADING ENGLISH SPACY \n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADVP -> ADV \n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.italian.raw(europarl_raw.italian.fileids()[0]))\n",
    "file = [\n",
    "    \"Il gatto grasso sta saltando.\",\n",
    "    \"Il gatto rosso è blu.\",\n",
    "    \"Il gatto sta correndo via\",\n",
    "    \"Amo i gatti,\",\n",
    "    \"I gatti piccoli sono fantastici.\",\n",
    "    \"I gatti grassi sono fantastici.\"\n",
    "]\n",
    "\n",
    "#LOADING ITALIAN SPACY \n",
    "nlp = spacy.load(\"it_core_news_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "ADVP -> ADV \n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.german.raw(europarl_raw.german.fileids()[0]))\n",
    "file = [ \n",
    "    \"Die fette Katze springt.\",\n",
    "    \"Die rote Katze ist blau.\",\n",
    "    \"Die Katze rennt davon.\",\n",
    "    \"Ich liebe Katzen.\",\n",
    "    \"Kleine Katzen sind toll.\",\n",
    "    \"Fette Katzen sind großartig.\"\n",
    "]\n",
    "\n",
    "#LOADING GERMAN SPACY \n",
    "nlp = spacy.load(\"de_core_news_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "ADVP -> ADV \n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.french.raw(europarl_raw.french.fileids()[0]))\n",
    "file = [\n",
    "    \"Le gros chat saute.\",\n",
    "    \"Le chat rouge est bleu.\",\n",
    "    \"Le chat s'enfuit.\",\n",
    "    \"J'aime les chats.\",\n",
    "    \"Les petits chats sont géniaux.\",\n",
    "    \"Les gros chats sont géniaux.\"\n",
    "]\n",
    "\n",
    "#LOADING FRENCH SPACY \n",
    "nlp = spacy.load(\"fr_core_news_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> PRON | NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "ADVP -> ADV \n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST PIPELINE:\n",
    "( No treebank parser for exctracting grammar)\n",
    "For each sentence in the file do the POS tagging and save the results in a map with every word saved in a list labeled with the Tag.\n",
    "Then transform the grammar into an nltk grammar object and use it inside the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m file[:\u001b[39m10\u001b[39m]: \n\u001b[1;32m      2\u001b[0m     possible_pos \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m      3\u001b[0m     grammar \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for sentence in file: \n",
    "    possible_pos = set()\n",
    "    grammar = {}\n",
    "    spacy_parsed_sent= nlp(sentence)\n",
    "    for token in spacy_parsed_sent:\n",
    "        possible_pos.add(token.pos_)\n",
    "        if not token.pos_ in grammar:\n",
    "            grammar[token.pos_] = []\n",
    "        word = '\"' + token.text + '\"'\n",
    "        if word not in grammar[token.pos_]:\n",
    "            grammar[token.pos_].append(word)\n",
    "\n",
    "    # Target types \n",
    "\n",
    "    grammar_rules = base_grammar\n",
    "    for type in possible_pos:  \n",
    "        appo_string = f\"{type} -> \"\n",
    "        index = len(grammar[type]) - 1\n",
    "        for word in grammar[type][0:index]:\n",
    "            appo_string+= \" {} |\".format(word)\n",
    "        appo_string+= \" {}\\n\".format(grammar[type][-1])\n",
    "        grammar_rules+= appo_string \n",
    "\n",
    "    print(f\"{sentence}\\n\")\n",
    "    nltk_grammar = nltk.CFG.fromstring(grammar_rules)\n",
    "    # print(f\"Sentence Grammar: {nltk_grammar}\")\n",
    "    parser = nltk.ChartParser(nltk_grammar)\n",
    "\n",
    "    spacy_tokenized = list(map(lambda e:e.text,spacy_parsed_sent))\n",
    "    for tree in parser.parse(spacy_tokenized[1:]):\n",
    "        print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textClassifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d20ef661449704aa7c8fdea66968a0854fe99004add1966f73a28e54fb6cfb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
