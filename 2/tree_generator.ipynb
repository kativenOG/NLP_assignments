{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignement 2\n",
    "\n",
    "The assignment consists in the development, in NLTK, OpenNLP, SketchEngine or GATE/Annie a pipeline that, starting from a text in input, in a given language (English, French, German and Italian are admissible) outputs the syntactic tree of the sentence itself, intended as a tree with root in S for sentence, and leaves on the tokens labelled with a single Part-of-speech. The generation of the tree can pass through one of the following models:\n",
    "\n",
    "1) PURE SYMBOLIC. The tree is generated by a LR analysis with CF LL2 grammar as a base. Candidates can assume the following:\n",
    "\n",
    "   a) Adjectives in English and German shall be only prefixed to nouns, whilst in French and Italian are only suffixed;\n",
    "\n",
    "    b) Verbs are all at present tense;\n",
    "\n",
    "    c) No pronouns are admitted;\n",
    "\n",
    "    d) Only one adverb is admitted, always post-poned with respect to the verb (independently of the language, and the type of adverb);\n",
    "\n",
    "    Overall the point above map a system that could be devised in regular expressions, but a Context-free grammar would be simpler to     \n",
    "    define. Candidate can either define a system by themselves or use a syntactic tree generation system that can be found on GitHub. \n",
    "    Same happens for POS-tagging, where some of the above mentioned systems can be customized by existing techniques that are available\n",
    "    in several fashions (including a pre-defined NLTK and OpenNLP libraries for POS-tagging and a module in GATE for the same purpose. Ambiguity \n",
    "    should be blocked onto first admissible tree.\n",
    "\n",
    "2) PURE ML. Candidates can develop a PLM with one-step Markov chains to forecast the following token, and used to generate the forecast of the\n",
    "     POS tags to be attributed. In this case the PLM can be generated starting with a Corpus, that could be obtained online, for instance by \n",
    "     using the Wikipedia access API, or other available free repos (including those available with SketchEngine. In this approach, candidates should\n",
    "     never use the forecasting to approach the determination of outcomes (for this would be identical purpose of distinguishing EN/non ENG (and\n",
    "     then IT/non IT, FR/not FR or DE/not DE) but only to identify the POS model in a sequence. In this case, the candidate should output the most\n",
    "     likely POS tagging, without associating the sequence to a tree in a direct fashion.\n",
    "\n",
    "Candidates are free to employ PURE ML approach to simplify, or pre-process the text in order to improve the performance of a PURE SYMBOLIC approach while generating a mixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /home/kativen/nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n",
      "[nltk_data] Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]     /home/kativen/nltk_data...\n",
      "[nltk_data]   Package universal_treebanks_v20 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#************************ GENERAL IMPORTS ************************#\n",
    "import spacy\n",
    "import nltk; nltk.download(\"europarl_raw\"); \n",
    "from tqdm import tqdm \n",
    "import math,random,pathlib,collections\n",
    "from nltk.corpus import europarl_raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "file = nltk.sent_tokenize(europarl_raw.english.raw(europarl_raw.english.fileids()[0]))\n",
    "# LOADING ENGLISH SPACY \n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= {\"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> PRON | NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADVP -> ADV | ADV ADVP\n",
    "PP -> P NP\n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PUNCT -> \"PUNCT\"\n",
    "SCONJ -> \"SCONJ\"\n",
    "PRON -> \"PRON\"\n",
    "SYM -> \"SYM\"\n",
    "NUM -> \"NUM\"\n",
    "N -> \"NOUN\"\n",
    "V -> \"VERB\"\n",
    "AUX -> \"AUX\"\n",
    "P -> \"ADP\"\n",
    "ADJ -> \"ADJ\"\n",
    "ADV -> \"ADV\"\n",
    "PUNCT -> '\"' | \"'\" | \".\" | \",\" | \":\" | \";\" | \"?\" | \"!\" | \"-\" | \"(\" | \")\" | \"...\" | \"....\" \n",
    "SPACE -> \" \"\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "file = nltk.sent_tokenize(europarl_raw.italian.raw(europarl_raw.italian.fileids()[0]))\n",
    "#LOADING ITALIAN SPACY \n",
    "nlp = spacy.load(\"it_core_news_sm\") \n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= {\"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> PRON | NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADVP -> ADV | ADV ADVP\n",
    "PP -> P NP\n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PUNCT -> \"PUNCT\"\n",
    "SCONJ -> \"SCONJ\"\n",
    "PRON -> \"PRON\"\n",
    "SYM -> \"SYM\"\n",
    "NUM -> \"NUM\"\n",
    "N -> \"NOUN\"\n",
    "V -> \"VERB\"\n",
    "AUX -> \"AUX\"\n",
    "P -> \"ADP\"\n",
    "ADJ -> \"ADJ\"\n",
    "ADV -> \"ADV\"\n",
    "PUNCT -> '\"' | \"'\" | \".\" | \",\" | \":\" | \";\" | \"?\" | \"!\" | \"-\" | \"(\" | \")\" | \"...\" | \"....\" \n",
    "SPACE -> \" \"\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "file = nltk.sent_tokenize(europarl_raw.german.raw(europarl_raw.german.fileids()[0]))\n",
    "#LOADING GERMAN SPACY \n",
    "nlp = spacy.load(\"de_core_news_sm\") \n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= {\"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> PRON | NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADVP -> ADV | ADV ADVP\n",
    "PP -> P NP\n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PUNCT -> \"PUNCT\"\n",
    "SCONJ -> \"SCONJ\"\n",
    "PRON -> \"PRON\"\n",
    "SYM -> \"SYM\"\n",
    "NUM -> \"NUM\"\n",
    "N -> \"NOUN\"\n",
    "V -> \"VERB\"\n",
    "AUX -> \"AUX\"\n",
    "P -> \"ADP\"\n",
    "ADJ -> \"ADJ\"\n",
    "ADV -> \"ADV\"\n",
    "PUNCT -> '\"' | \"'\" | \".\" | \",\" | \":\" | \";\" | \"?\" | \"!\" | \"-\" | \"(\" | \")\" | \"...\" | \"....\" \n",
    "SPACE -> \" \"\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "file = nltk.sent_tokenize(europarl_raw.french.raw(europarl_raw.french.fileids()[0]))\n",
    "#LOADING FRENCH SPACY \n",
    "nlp = spacy.load(\"fr_core_news_sm\") \n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= {\"\"\"\n",
    "S -> NP VP PUNCT | NP VP\n",
    "NP -> PRON | NUM ADJ N | N\n",
    "VP -> V NP | V | V ADVP | VP SCONJ VP | AUX VP\n",
    "ADVP -> ADV | ADV ADVP\n",
    "PP -> P NP\n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PUNCT -> \"PUNCT\"\n",
    "SCONJ -> \"SCONJ\"\n",
    "PRON -> \"PRON\"\n",
    "SYM -> \"SYM\"\n",
    "NUM -> \"NUM\"\n",
    "N -> \"NOUN\"\n",
    "V -> \"VERB\"\n",
    "AUX -> \"AUX\"\n",
    "P -> \"ADP\"\n",
    "ADJ -> \"ADJ\"\n",
    "ADV -> \"ADV\"\n",
    "PUNCT -> '\"' | \"'\" | \".\" | \",\" | \":\" | \";\" | \"?\" | \"!\" | \"-\" | \"(\" | \")\" | \"...\" | \"....\" \n",
    "SPACE -> \" \"\n",
    "\"\"\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST PIPELINE:\n",
    "( No treebank parser for exctracting grammar)\n",
    "For each sentence in the file do the POS tagging and save the results in a map with every word saved in a list labeled with the Tag.\n",
    "Then transform the grammar into an nltk grammar object and use it inside the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Reprise de la session Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances .\n",
      "\n",
      "Comme vous avez pu le constater , le grand \" bogue de l' an 2000 \" ne s' est pas produit .\n",
      "\n",
      "En revanche , les citoyens d' un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles .\n",
      "\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours , au cours de cette période de session .\n",
      "\n",
      "En attendant , je souhaiterais , comme un certain nombre de collègues me l' ont demandé , que nous observions une minute de silence pour toutes les victimes , des tempêtes notamment , dans les différents pays de l' Union européenne qui ont été touchés .\n",
      "\n",
      "Je vous invite à vous lever pour cette minute de silence .\n",
      "\n",
      "( Le Parlement , debout , observe une minute de silence ) Madame la Présidente , c' est une motion de procédure .\n",
      "\n",
      "Vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au Sri Lanka .\n",
      "\n",
      "L' une des personnes qui vient d' être assassinée au Sri Lanka est M. Kumar Ponnambalam , qui avait rendu visite au Parlement européen il y a quelques mois à peine .\n",
      "\n",
      "Ne pensez-vous pas , Madame la Présidente , qu' il conviendrait d' écrire une lettre au président du Sri Lanka pour lui communiquer que le Parlement déplore les morts violentes , dont celle de M. Ponnambalam , et pour l' inviter instamment à faire tout ce qui est en son pouvoir pour chercher une réconciliation pacifique et mettre un terme à cette situation particulièrement difficile .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for sentence in file[:10]: \n",
    "    possible_pos = set()\n",
    "    grammar = {}\n",
    "    spacy_parsed_sent= nlp(sentence)\n",
    "    for token in spacy_parsed_sent:\n",
    "        possible_pos.add(token.pos_)\n",
    "        if not token.pos_ in grammar:\n",
    "            grammar[token.pos_] = []\n",
    "        word = '\"' + token.text + '\"'\n",
    "        if word not in grammar[token.pos_]:\n",
    "            grammar[token.pos_].append(word)\n",
    "\n",
    "    # Target types \n",
    "\n",
    "    grammar_rules = base_grammar\n",
    "    for type in possible_pos:  \n",
    "        if type != \"SPACE\" and type!=\"PUNCT\":\n",
    "            appo_string = f\"{type} -> \"\n",
    "            index = len(grammar[type]) - 1\n",
    "            for word in grammar[type][0:index]:\n",
    "                appo_string+= \" {} |\".format(word)\n",
    "            appo_string+= \" {}\\n\".format(grammar[type][-1])\n",
    "            grammar_rules+= appo_string \n",
    "\n",
    "    print(f\"{sentence}\\n\")\n",
    "    # print(f\"Grammar Rules: {grammar_rules}\")\n",
    "    nltk_grammar = nltk.CFG.fromstring(grammar_rules)\n",
    "    # print(f\"Sentence Grammar: {nltk_grammar}\")\n",
    "    parser = nltk.ChartParser(nltk_grammar)\n",
    "\n",
    "    spacy_tokenized = []\n",
    "    for token in spacy_parsed_sent:\n",
    "        if token.text != \"\\n\" and token.text != \"\\\\n\":\n",
    "            spacy_tokenized.append(token.text) \n",
    "    for tree in parser.parse(spacy_tokenized[1:]):\n",
    "        print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textClassifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d20ef661449704aa7c8fdea66968a0854fe99004add1966f73a28e54fb6cfb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
