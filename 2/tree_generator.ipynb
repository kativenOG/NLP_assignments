{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignement 2\n",
    "\n",
    "The assignment consists in the development, in NLTK, OpenNLP, SketchEngine or GATE/Annie a pipeline that, starting from a text in input, in a given language (English, French, German and Italian are admissible) outputs the syntactic tree of the sentence itself, intended as a tree with root in S for sentence, and leaves on the tokens labelled with a single Part-of-speech. The generation of the tree can pass through one of the following models:\n",
    "\n",
    "1) PURE SYMBOLIC. The tree is generated by a LR analysis with CF LL2 grammar as a base. Candidates can assume the following:\n",
    "\n",
    "   a) Adjectives in English and German shall be only prefixed to nouns, whilst in French and Italian are only suffixed;\n",
    "\n",
    "    b) Verbs are all at present tense;\n",
    "\n",
    "    c) No pronouns are admitted;\n",
    "\n",
    "    d) Only one adverb is admitted, always post-poned with respect to the verb (independently of the language, and the type of adverb);\n",
    "\n",
    "    Overall the point above map a system that could be devised in regular expressions, but a Context-free grammar would be simpler to     \n",
    "    define. Candidate can either define a system by themselves or use a syntactic tree generation system that can be found on GitHub. \n",
    "    Same happens for POS-tagging, where some of the above mentioned systems can be customized by existing techniques that are available\n",
    "    in several fashions (including a pre-defined NLTK and OpenNLP libraries for POS-tagging and a module in GATE for the same purpose. Ambiguity \n",
    "    should be blocked onto first admissible tree.\n",
    "\n",
    "2) PURE ML. Candidates can develop a PLM with one-step Markov chains to forecast the following token, and used to generate the forecast of the\n",
    "     POS tags to be attributed. In this case the PLM can be generated starting with a Corpus, that could be obtained online, for instance by \n",
    "     using the Wikipedia access API, or other available free repos (including those available with SketchEngine. In this approach, candidates should\n",
    "     never use the forecasting to approach the determination of outcomes (for this would be identical purpose of distinguishing EN/non ENG (and\n",
    "     then IT/non IT, FR/not FR or DE/not DE) but only to identify the POS model in a sequence. In this case, the candidate should output the most\n",
    "     likely POS tagging, without associating the sequence to a tree in a direct fashion.\n",
    "\n",
    "Candidates are free to employ PURE ML approach to simplify, or pre-process the text in order to improve the performance of a PURE SYMBOLIC approach while generating a mixed model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Symbolic:\n",
    "To resolve this assignment task i decided to use the Pure Symbolic approach. I later discovered that this task is composed of 3 main subtopics:\n",
    "1. Tokenize and do Part of Speech tagging for the input phrase in all 4 of the languages;\n",
    "2. Create a base grammar (for each one of languages) following the provided rules and add all the word-tag (terminals) to it, then transform it to a nltk-compatible version.\n",
    "3. With the nltk-grammar object create a parser used to generate a syntactic tree by parsing the phrase. If the parser finds more trees for a single phrase print only the first one;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /home/kativen/nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#************************ GENERAL IMPORTS ************************#\n",
    "import spacy\n",
    "import nltk \n",
    "spacy_to_nltk_gram = \"\"\"\n",
    "N -> NOUN\n",
    "V -> VERB\n",
    "P -> ADP\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1: Tokenization and POS tagging:\n",
    "To tokenize and perform pos tagging I used the library Spacy. Spacy provides a broadth catalogue of supported languages (far more than nltk) and it performs both the operation within just one function.\n",
    "Spacy, given an input text, returns an array of tokenized objects that also contain their tag as a field.\n",
    "I created one block for each one of the languages using the same variable names, to run any-one of the languages is as easy as just rerunning the language specific block."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.english.raw(europarl_raw.english.fileids()[0]))\n",
    "file = [\n",
    "    \"The fat cat is jumping.\",\n",
    "    \"The red cat is blue.\",\n",
    "    \"The cat is running away.\",\n",
    "    \"I love cats.\",\n",
    "    \"Small cats are awesome.\",\n",
    "    \"Fat cats are awesome.\"\n",
    "]\n",
    "\n",
    "# LOADING ENGLISH SPACY \n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP | PUNCT NP VP PUNCT\n",
    "NP -> NUM ADJ N | N | ADJP NP  | DET NP \n",
    "VP -> VP NP | V | VP ADVP | VP SCONJ VP | AUX VP | VP PUNCT | AUX ADJP| AUX ADV \n",
    "ADVP -> ADV \n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.italian.raw(europarl_raw.italian.fileids()[0]))\n",
    "file = [\n",
    "    \"Il gatto grasso sta saltando.\",\n",
    "    \"Il gatto rosso è blu.\",\n",
    "    \"Il gatto sta correndo via\",\n",
    "    \"Amo i gatti,\",\n",
    "    \"I gatti piccoli sono fantastici.\",\n",
    "    \"I gatti grassi sono fantastici.\"\n",
    "]\n",
    "\n",
    "#LOADING ITALIAN SPACY \n",
    "nlp = spacy.load(\"it_core_news_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP | PUNCT NP VP PUNCT\n",
    "NP -> NUM N ADJ | N | NP ADJP | DET NP \n",
    "VP -> VP NP | V | VP ADVP | VP SCONJ VP | AUX VP | VP PUNCT | AUX ADJP| AUX ADV \n",
    "ADVP -> ADV \n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.german.raw(europarl_raw.german.fileids()[0]))\n",
    "file = [ \n",
    "    \"Die fette Katze springt.\",\n",
    "    \"Die rote Katze ist blau.\",\n",
    "    \"Die Katze rennt davon.\",\n",
    "    \"Ich liebe Katzen.\",\n",
    "    \"Kleine Katzen sind toll.\",\n",
    "    \"Fette Katzen sind großartig.\"\n",
    "]\n",
    "\n",
    "#LOADING GERMAN SPACY \n",
    "nlp = spacy.load(\"de_core_news_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP | PUNCT NP VP PUNCT\n",
    "NP -> NUM ADJ N | N | ADJP NP  | DET NP \n",
    "VP -> VP NP | V | VP ADVP | VP SCONJ VP | AUX VP | VP PUNCT | AUX ADJP| AUX ADV \n",
    "ADVP -> ADV \n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATA\n",
    "# file = nltk.sent_tokenize(europarl_raw.french.raw(europarl_raw.french.fileids()[0]))\n",
    "file = [\n",
    "    \"Le gros chat saute.\",\n",
    "    \"Le chat rouge est bleu.\",\n",
    "    \"Le chat s'enfuit.\",\n",
    "    \"J'aime les chats.\",\n",
    "    \"Les petits chats sont géniaux.\",\n",
    "    \"Les gros chats sont géniaux.\"\n",
    "]\n",
    "\n",
    "#LOADING FRENCH SPACY \n",
    "nlp = spacy.load(\"fr_core_news_sm\") \n",
    "\n",
    "# LANGUAGE SPECIFIC GRAMMAR  \n",
    "base_grammar= \"\"\"\n",
    "S -> NP VP PUNCT | NP VP | PUNCT NP VP PUNCT\n",
    "NP -> NUM N ADJ | N | NP ADJP | DET NP \n",
    "VP -> VP NP | V | VP ADVP | VP SCONJ VP | AUX VP | VP PUNCT | AUX ADJP | AUX ADV \n",
    "ADVP -> ADV \n",
    "ADJP -> ADJ | ADJ ADJP\n",
    "PP -> P NP\n",
    "\"\"\" + spacy_to_nltk_gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Creating a NLTK-compatible Grammar\n",
    "I created a phrase-specific grammar by just adding to the language specific grammar strings containing the Tag-Word combination, for each word in the phrase. <br/>\n",
    "To convert this string to grammar I used the **nltk.CFG.fromstring** function, and then use the return value (a nltk-grammar object) to create a phrase specific parser."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a Parser and Generate syntactic Trees\n",
    "The parser returns a list of compatible trees, that reppresent all the possible combination in which the phrase can be parsed. <br/>\n",
    "The input phrases aren't all parsable with the given base grammar: this shows the limitations of the provided grammar and of this method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die fette Katze springt.\n",
      "\n",
      "Die -> DET\n",
      "fette -> ADJ\n",
      "Katze -> NOUN\n",
      "springt -> VERB\n",
      ". -> PUNCT\n",
      "\n",
      "\n",
      "(S\n",
      "  (NP (DET Die) (NP (ADJP (ADJ fette)) (NP (N (NOUN Katze)))))\n",
      "  (VP (V (VERB springt)))\n",
      "  (PUNCT .))\n",
      "\n",
      "\n",
      "\n",
      "Die rote Katze ist blau.\n",
      "\n",
      "Die -> DET\n",
      "rote -> ADJ\n",
      "Katze -> NOUN\n",
      "ist -> AUX\n",
      "blau -> ADV\n",
      ". -> PUNCT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Die Katze rennt davon.\n",
      "\n",
      "Die -> DET\n",
      "Katze -> NOUN\n",
      "rennt -> VERB\n",
      "davon -> ADV\n",
      ". -> PUNCT\n",
      "\n",
      "\n",
      "(S\n",
      "  (NP (DET Die) (NP (N (NOUN Katze))))\n",
      "  (VP (VP (V (VERB rennt))) (ADVP (ADV davon)))\n",
      "  (PUNCT .))\n",
      "\n",
      "\n",
      "\n",
      "Ich liebe Katzen.\n",
      "\n",
      "Ich -> PRON\n",
      "liebe -> ADJ\n",
      "Katzen -> NOUN\n",
      ". -> PUNCT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Kleine Katzen sind toll.\n",
      "\n",
      "Kleine -> ADJ\n",
      "Katzen -> NOUN\n",
      "sind -> AUX\n",
      "toll -> ADV\n",
      ". -> PUNCT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fette Katzen sind großartig.\n",
      "\n",
      "Fette -> NOUN\n",
      "Katzen -> NOUN\n",
      "sind -> AUX\n",
      "großartig -> ADV\n",
      ". -> PUNCT\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in file: \n",
    "    possible_pos = set()\n",
    "    grammar = {}\n",
    "    spacy_parsed_sent= nlp(sentence)\n",
    "    print(f\"{sentence}\\n\")\n",
    "    for token in spacy_parsed_sent:\n",
    "        print(f\"{token.text } -> {token.pos_}\")\n",
    "        possible_pos.add(token.pos_)\n",
    "        if not token.pos_ in grammar:\n",
    "            grammar[token.pos_] = []\n",
    "        word = '\"' + token.text + '\"'\n",
    "        if word not in grammar[token.pos_]:\n",
    "            grammar[token.pos_].append(word)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    # Target types \n",
    "\n",
    "    grammar_rules = base_grammar\n",
    "    for type in possible_pos:  \n",
    "        appo_string = f\"{type} -> \"\n",
    "        index = len(grammar[type]) - 1\n",
    "        for word in grammar[type][0:index]:\n",
    "            appo_string+= \" {} |\".format(word)\n",
    "        appo_string+= \" {}\\n\".format(grammar[type][-1])\n",
    "        grammar_rules+= appo_string \n",
    "\n",
    "    nltk_grammar = nltk.CFG.fromstring(grammar_rules)\n",
    "    # print(f\"Sentence Grammar: {nltk_grammar}\\n\")\n",
    "    parser = nltk.ChartParser(nltk_grammar)\n",
    "\n",
    "    spacy_tokenized = list(map(lambda e:e.text,spacy_parsed_sent))\n",
    "    # print(f\"Spacy Tokenized: {spacy_tokenized}\\n\")\n",
    "    trees = list(parser.parse(spacy_tokenized))\n",
    "    if trees: print(trees[0]) \n",
    "    # for tree in parser.parse(spacy_tokenized):\n",
    "    #     print(tree)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textClassifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d20ef661449704aa7c8fdea66968a0854fe99004add1966f73a28e54fb6cfb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
