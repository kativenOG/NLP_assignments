{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignement 2\n",
    "\n",
    "The assignment consists in the development, in NLTK, OpenNLP, SketchEngine or GATE/Annie a pipeline that, starting from a text in input, in a given language (English, French, German and Italian are admissible) outputs the syntactic tree of the sentence itself, intended as a tree with root in S for sentence, and leaves on the tokens labelled with a single Part-of-speech. The generation of the tree can pass through one of the following models:\n",
    "\n",
    "1) PURE SYMBOLIC. The tree is generated by a LR analysis with CF LL2 grammar as a base. Candidates can assume the following:\n",
    "\n",
    "   a) Adjectives in English and German shall be only prefixed to nouns, whilst in French and Italian are only suffixed;\n",
    "\n",
    "    b) Verbs are all at present tense;\n",
    "\n",
    "    c) No pronouns are admitted;\n",
    "\n",
    "    d) Only one adverb is admitted, always post-poned with respect to the verb (independently of the language, and the type of adverb);\n",
    "\n",
    "    Overall the point above map a system that could be devised in regular expressions, but a Context-free grammar would be simpler to     \n",
    "    define. Candidate can either define a system by themselves or use a syntactic tree generation system that can be found on GitHub. \n",
    "    Same happens for POS-tagging, where some of the above mentioned systems can be customized by existing techniques that are available\n",
    "    in several fashions (including a pre-defined NLTK and OpenNLP libraries for POS-tagging and a module in GATE for the same purpose. Ambiguity \n",
    "    should be blocked onto first admissible tree.\n",
    "\n",
    "2) PURE ML. Candidates can develop a PLM with one-step Markov chains to forecast the following token, and used to generate the forecast of the\n",
    "     POS tags to be attributed. In this case the PLM can be generated starting with a Corpus, that could be obtained online, for instance by \n",
    "     using the Wikipedia access API, or other available free repos (including those available with SketchEngine. In this approach, candidates should\n",
    "     never use the forecasting to approach the determination of outcomes (for this would be identical purpose of distinguishing EN/non ENG (and\n",
    "     then IT/non IT, FR/not FR or DE/not DE) but only to identify the POS model in a sequence. In this case, the candidate should output the most\n",
    "     likely POS tagging, without associating the sequence to a tree in a direct fashion.\n",
    "\n",
    "Candidates are free to employ PURE ML approach to simplify, or pre-process the text in order to improve the performance of a PURE SYMBOLIC approach while generating a mixed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package europarl_raw to\n",
      "[nltk_data]     /home/kativen/nltk_data...\n",
      "[nltk_data]   Package europarl_raw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#************************ GENERAL IMPORTS ************************#\n",
    "import spacy\n",
    "import nltk; nltk.download(\"europarl_raw\")\n",
    "from tqdm import tqdm \n",
    "import math,random,pathlib,collections\n",
    "from nltk.corpus import europarl_raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING ENGLISH SPACY \n",
    "file = nltk.sent_tokenize(europarl_raw.english.raw(europarl_raw.english.fileids()[0]))\n",
    "nlp = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING ITALIAN SPACY \n",
    "file = nltk.sent_tokenize(europarl_raw.italian.raw(europarl_raw.italian.fileids()[0]))\n",
    "nlp = spacy.load(\"it_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING GERMAN SPACY \n",
    "file = nltk.sent_tokenize(europarl_raw.german.raw(europarl_raw.german.fileids()[0]))\n",
    "nlp = spacy.load(\"de_core_news_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING FRENCH SPACY \n",
    "file = nltk.sent_tokenize(europarl_raw.french.raw(europarl_raw.french.fileids()[0]))\n",
    "nlp = spacy.load(\"fr_core_news_sm\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Esempio libro nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (Det the) (N cat)) (VP (V chased) (NP (Det the) (N dog))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### EXAMPLE  FROM NLTK BOOK \n",
    "\n",
    "# Generate a CSP Grammar with nltk \n",
    "eng_grammar = nltk.CFG.fromstring(\"\"\" # this is for an example usage\n",
    "S -> NP VP\n",
    "NP -> Det N\n",
    "VP -> V NP\n",
    "Det -> 'the'\n",
    "N -> 'cat' | 'dog'\n",
    "V -> 'chased' | 'sat'\n",
    "\"\"\") # Takes a File as an input and returns a nltk.CFG object \n",
    "\n",
    "parser = nltk.BottomUpLeftCornerChartParser(grammar=eng_grammar)\n",
    "sentence = \"the cat chased the dog\".split()\n",
    "trees = parser.parse(sentence)\n",
    "for tree in trees: \n",
    "    print(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAVORO EFFETTIVO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  \n",
      "Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .\n",
      "\n",
      "Grammar Rules: S -> NP VP\n",
      "PP -> P NP\n",
      "NP -> Det N | Det N PP | 'I'\n",
      "VP -> V NP | VP PP\n",
      "SPACE -> ' '\n",
      "NOUN ->  \"session\" | \"year\" | \"hope\" | \"period\"\n",
      "CCONJ ->  \"and\"\n",
      "ADJ ->  \"happy\" | \"new\" | \"pleasant\" | \"festive\"\n",
      "VERB ->  \"declare\" | \"resumed\" | \"adjourned\" | \"like\" | \"wish\" | \"enjoyed\"\n",
      "PART ->  \"to\"\n",
      "ADP ->  \"of\" | \"on\" | \"in\"\n",
      "AUX ->  \"would\"\n",
      "NUM ->  \"17\" | \"1999\"\n",
      "SCONJ ->  \"that\"\n",
      "PUNCT ->  \",\" | \".\"\n",
      "ADV ->  \"once\" | \"again\"\n",
      "PRON ->  \"I\" | \"you\"\n",
      "PROPN ->  \"Resumption\" | \"European\" | \"Parliament\" | \"Friday\" | \"December\"\n",
      "DET ->  \"the\" | \"a\"\n",
      "\n",
      "Sentence: Although , as you will have seen , the dreaded ' millennium bug ' failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful .\n",
      "\n",
      "Grammar Rules: S -> NP VP\n",
      "PP -> P NP\n",
      "NP -> Det N | Det N PP | 'I'\n",
      "VP -> V NP | VP PP\n",
      "SPACE -> ' '\n",
      "NOUN ->  \"session\" | \"year\" | \"hope\" | \"period\"\n",
      "CCONJ ->  \"and\"\n",
      "ADJ ->  \"happy\" | \"new\" | \"pleasant\" | \"festive\"\n",
      "VERB ->  \"declare\" | \"resumed\" | \"adjourned\" | \"like\" | \"wish\" | \"enjoyed\"\n",
      "PART ->  \"to\"\n",
      "ADP ->  \"of\" | \"on\" | \"in\"\n",
      "AUX ->  \"would\"\n",
      "NUM ->  \"17\" | \"1999\"\n",
      "SCONJ ->  \"that\"\n",
      "PUNCT ->  \",\" | \".\"\n",
      "ADV ->  \"once\" | \"again\"\n",
      "PRON ->  \"I\" | \"you\"\n",
      "PROPN ->  \"Resumption\" | \"European\" | \"Parliament\" | \"Friday\" | \"December\"\n",
      "DET ->  \"the\" | \"a\"\n",
      "SPACE -> ' '\n",
      "NOUN ->  \"millennium\" | \"bug\" | \"people\" | \"number\" | \"countries\" | \"series\" | \"disasters\"\n",
      "ADJ ->  \"natural\" | \"dreadful\"\n",
      "VERB ->  \"seen\" | \"dreaded\" | \"failed\" | \"materialise\" | \"suffered\"\n",
      "PART ->  \"to\"\n",
      "ADP ->  \"in\" | \"of\"\n",
      "AUX ->  \"will\" | \"have\" | \"were\"\n",
      "SCONJ ->  \"Although\" | \"as\"\n",
      "PUNCT ->  \",\" | \"'\" | \".\"\n",
      "ADV ->  \"still\" | \"truly\"\n",
      "PRON ->  \"you\" | \"that\"\n",
      "DET ->  \"the\" | \"a\"\n",
      "\n",
      "Sentence: You have requested a debate on this subject in the course of the next few days , during this part-session .\n",
      "\n",
      "Grammar Rules: S -> NP VP\n",
      "PP -> P NP\n",
      "NP -> Det N | Det N PP | 'I'\n",
      "VP -> V NP | VP PP\n",
      "SPACE -> ' '\n",
      "NOUN ->  \"session\" | \"year\" | \"hope\" | \"period\"\n",
      "CCONJ ->  \"and\"\n",
      "ADJ ->  \"happy\" | \"new\" | \"pleasant\" | \"festive\"\n",
      "VERB ->  \"declare\" | \"resumed\" | \"adjourned\" | \"like\" | \"wish\" | \"enjoyed\"\n",
      "PART ->  \"to\"\n",
      "ADP ->  \"of\" | \"on\" | \"in\"\n",
      "AUX ->  \"would\"\n",
      "NUM ->  \"17\" | \"1999\"\n",
      "SCONJ ->  \"that\"\n",
      "PUNCT ->  \",\" | \".\"\n",
      "ADV ->  \"once\" | \"again\"\n",
      "PRON ->  \"I\" | \"you\"\n",
      "PROPN ->  \"Resumption\" | \"European\" | \"Parliament\" | \"Friday\" | \"December\"\n",
      "DET ->  \"the\" | \"a\"\n",
      "SPACE -> ' '\n",
      "NOUN ->  \"millennium\" | \"bug\" | \"people\" | \"number\" | \"countries\" | \"series\" | \"disasters\"\n",
      "ADJ ->  \"natural\" | \"dreadful\"\n",
      "VERB ->  \"seen\" | \"dreaded\" | \"failed\" | \"materialise\" | \"suffered\"\n",
      "PART ->  \"to\"\n",
      "ADP ->  \"in\" | \"of\"\n",
      "AUX ->  \"will\" | \"have\" | \"were\"\n",
      "SCONJ ->  \"Although\" | \"as\"\n",
      "PUNCT ->  \",\" | \"'\" | \".\"\n",
      "ADV ->  \"still\" | \"truly\"\n",
      "PRON ->  \"you\" | \"that\"\n",
      "DET ->  \"the\" | \"a\"\n",
      "SPACE -> ' '\n",
      "NOUN ->  \"debate\" | \"subject\" | \"course\" | \"days\" | \"part\" | \"session\"\n",
      "ADJ ->  \"next\" | \"few\"\n",
      "VERB ->  \"requested\"\n",
      "ADP ->  \"on\" | \"in\" | \"of\" | \"during\"\n",
      "AUX ->  \"have\"\n",
      "PUNCT ->  \",\" | \"-\" | \".\"\n",
      "PRON ->  \"You\"\n",
      "DET ->  \"a\" | \"this\" | \"the\"\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"'part-session'\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m parser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mChartParser(nltk_grammar)\n\u001b[1;32m     38\u001b[0m sentence \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(sentence)\n\u001b[0;32m---> 39\u001b[0m \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39;49mparse(sentence):\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(tree)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/parse/chart.py:1474\u001b[0m, in \u001b[0;36mChartParser.parse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, tokens, tree_class\u001b[39m=\u001b[39mTree):\n\u001b[0;32m-> 1474\u001b[0m     chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchart_parse(tokens)\n\u001b[1;32m   1475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(chart\u001b[39m.\u001b[39mparses(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\u001b[39m.\u001b[39mstart(), tree_class\u001b[39m=\u001b[39mtree_class))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/parse/chart.py:1432\u001b[0m, in \u001b[0;36mChartParser.chart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1429\u001b[0m trace_new_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trace_new_edges\n\u001b[1;32m   1431\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tokens)\n\u001b[0;32m-> 1432\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grammar\u001b[39m.\u001b[39;49mcheck_coverage(tokens)\n\u001b[1;32m   1433\u001b[0m chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chart_class(tokens)\n\u001b[1;32m   1434\u001b[0m grammar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/grammar.py:665\u001b[0m, in \u001b[0;36mCFG.check_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mif\u001b[39;00m missing:\n\u001b[1;32m    664\u001b[0m     missing \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mw\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m missing)\n\u001b[0;32m--> 665\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGrammar does not cover some of the \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput words: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m missing\n\u001b[1;32m    667\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"'part-session'\"."
     ]
    }
   ],
   "source": [
    "grammar_rules = \"S -> NP VP\\nPP -> P NP\\nNP -> Det N | Det N PP | 'I'\\nVP -> V NP | VP PP\\n\"\n",
    "\n",
    "# For each sentence in the file do the POS tagging and save the results in a map with every word saved in a list labeled with the Tag.\n",
    "for sentence in file: \n",
    "    possible_pos = set()\n",
    "    grammar = {}\n",
    "    spacy_parsed_sent= nlp(sentence)\n",
    "    for token in spacy_parsed_sent:\n",
    "        possible_pos.add(token.pos_)\n",
    "        if not token.pos_ in grammar:\n",
    "            grammar[token.pos_] = []\n",
    "        word = '\"' + token.text + '\"'\n",
    "        if word not in grammar[token.pos_]:\n",
    "            grammar[token.pos_].append(word)\n",
    "\n",
    "    # Target types \n",
    "    grammar_rules+= \"SPACE -> ' '\\n\" \n",
    "    for type in possible_pos:  \n",
    "        if type != \"SPACE\":\n",
    "            appo_string = f\"{type} -> \"\n",
    "            index = len(grammar[type]) - 1\n",
    "            for word in grammar[type][0:index]:\n",
    "                appo_string+= \" {} |\".format(word)\n",
    "            appo_string+= \" {}\\n\".format(grammar[type][-1])\n",
    "            grammar_rules+= appo_string \n",
    "\n",
    "    # if \"NOUN\" in possible_pos and \"PROPN\" in possible_pos:\n",
    "    #     grammar_rules += f'N -> {\" | \".join(grammar[\"NOUN\"] + grammar[\"PROPN\"])}\\n'\n",
    "    # if \"VERB\" in possible_pos and \"AUX\" in possible_pos:\n",
    "    #     grammar_rules += f'N -> {\" | \".join(grammar[\"VERB\"] + grammar[\"AUX\"])}\\n'\n",
    "\n",
    "    print(f\"Sentence: {sentence}\\n\")\n",
    "    print(f\"Grammar Rules: {grammar_rules}\")\n",
    "    nltk_grammar = nltk.CFG.fromstring(grammar_rules)\n",
    "    # print(f\"Sentence Grammar: {nltk_grammar}\")\n",
    "\n",
    "    parser = nltk.ChartParser(nltk_grammar)\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    for tree in parser.parse(sentence):\n",
    "        print(tree)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent:  ['Although', ',', 'as', 'you', 'will', 'have', 'seen', ',', 'the', 'dreaded', \"'\", 'millennium', 'bug', \"'\", 'failed', 'to', 'materialise', ',', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful', '.']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: '\\'Although\\', \\'as\\', \\'will\\', \\'have\\', \\'seen\\', \\'dreaded\\', \"\\'\", \\'millennium\\', \\'bug\\', \"\\'\", \\'failed\\', \\'materialise\\', \\'still\\', \\'people\\', \\'number\\', \\'countries\\', \\'suffered\\', \\'series\\', \\'natural\\', \\'disasters\\', \\'truly\\', \\'were\\', \\'dreadful\\''.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(sentence)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msent: \u001b[39m\u001b[39m\"\u001b[39m, sentence)\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39;49mparse(sentence):\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(tree)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/parse/chart.py:1474\u001b[0m, in \u001b[0;36mChartParser.parse\u001b[0;34m(self, tokens, tree_class)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, tokens, tree_class\u001b[39m=\u001b[39mTree):\n\u001b[0;32m-> 1474\u001b[0m     chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchart_parse(tokens)\n\u001b[1;32m   1475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(chart\u001b[39m.\u001b[39mparses(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\u001b[39m.\u001b[39mstart(), tree_class\u001b[39m=\u001b[39mtree_class))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/parse/chart.py:1432\u001b[0m, in \u001b[0;36mChartParser.chart_parse\u001b[0;34m(self, tokens, trace)\u001b[0m\n\u001b[1;32m   1429\u001b[0m trace_new_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trace_new_edges\n\u001b[1;32m   1431\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tokens)\n\u001b[0;32m-> 1432\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grammar\u001b[39m.\u001b[39;49mcheck_coverage(tokens)\n\u001b[1;32m   1433\u001b[0m chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chart_class(tokens)\n\u001b[1;32m   1434\u001b[0m grammar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/grammar.py:665\u001b[0m, in \u001b[0;36mCFG.check_coverage\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mif\u001b[39;00m missing:\n\u001b[1;32m    664\u001b[0m     missing \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mw\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m missing)\n\u001b[0;32m--> 665\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGrammar does not cover some of the \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput words: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m missing\n\u001b[1;32m    667\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Grammar does not cover some of the input words: '\\'Although\\', \\'as\\', \\'will\\', \\'have\\', \\'seen\\', \\'dreaded\\', \"\\'\", \\'millennium\\', \\'bug\\', \"\\'\", \\'failed\\', \\'materialise\\', \\'still\\', \\'people\\', \\'number\\', \\'countries\\', \\'suffered\\', \\'series\\', \\'natural\\', \\'disasters\\', \\'truly\\', \\'were\\', \\'dreadful\\''."
     ]
    }
   ],
   "source": [
    "sentence = nltk.word_tokenize(sentence)\n",
    "print(\"sent: \", sentence)\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar Rules: S -> NP VP\n",
      "PP -> P NP\n",
      "NP -> Det N | Det N PP | 'I'\n",
      "VP -> V NP | VP PP\n",
      "SPACE -> ' '\n",
      "PRON ->  'A'\n",
      "SPACE -> ' '\n",
      "X ->  'l'\n",
      "SPACE -> ' '\n",
      "INTJ ->  't'\n",
      "SPACE -> ' '\n",
      "X ->  'h'\n",
      "SPACE -> ' '\n",
      "INTJ ->  'o'\n",
      "SPACE -> ' '\n",
      "NOUN ->  'u'\n",
      "SPACE -> ' '\n",
      "PROPN ->  'g'\n",
      "SPACE -> ' '\n",
      "X ->  'h'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "PUNCT ->  ','\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "PRON ->  'a'\n",
      "SPACE -> ' '\n",
      "NOUN ->  's'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "X ->  'y'\n",
      "SPACE -> ' '\n",
      "INTJ ->  'o'\n",
      "SPACE -> ' '\n",
      "NOUN ->  'u'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "PROPN ->  'w'\n",
      "SPACE -> ' '\n",
      "PRON ->  'i'\n",
      "SPACE -> ' '\n",
      "X ->  'l'\n",
      "SPACE -> ' '\n",
      "X ->  'l'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "X ->  'h'\n",
      "SPACE -> ' '\n",
      "PRON ->  'a'\n",
      "SPACE -> ' '\n",
      "X ->  'v'\n",
      "SPACE -> ' '\n",
      "X ->  'e'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "NOUN ->  's'\n",
      "SPACE -> ' '\n",
      "X ->  'e'\n",
      "SPACE -> ' '\n",
      "X ->  'e'\n",
      "SPACE -> ' '\n",
      "CCONJ ->  'n'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "PUNCT ->  ','\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "INTJ ->  't'\n",
      "SPACE -> ' '\n",
      "X ->  'h'\n",
      "SPACE -> ' '\n",
      "X ->  'e'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "X ->  'd'\n",
      "SPACE -> ' '\n",
      "X ->  'r'\n",
      "SPACE -> ' '\n",
      "X ->  'e'\n",
      "SPACE -> ' '\n",
      "PRON ->  'a'\n",
      "SPACE -> ' '\n",
      "X ->  'd'\n",
      "SPACE -> ' '\n",
      "X ->  'e'\n",
      "SPACE -> ' '\n",
      "X ->  'd'\n",
      "SPACE -> ' '\n",
      "SPACE -> ' '\n",
      "PUNCT ->  '''\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textClassifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d20ef661449704aa7c8fdea66968a0854fe99004add1966f73a28e54fb6cfb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
